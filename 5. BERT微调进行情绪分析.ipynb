{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d54218",
   "metadata": {},
   "source": [
    "# 1. load lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c874f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #os模块提供的就是各种 Python 程序与操作系统进行交互的接口\n",
    "import re #re模块是python独有的匹配字符串的模块，该模块中提供正则表达式，用来提取字符串\n",
    "from tqdm import tqdm # tqdm 用来显示进度条\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16be1367",
   "metadata": {},
   "source": [
    "# 2. Data\n",
    "\n",
    "## 2.1 download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f5e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "import requests\n",
    "request = requests.get(\"https://drive.google.com/uc?export=download&id=1wHt8PsMLsfX5yNSqrt2fSTcb8LEiclcf\")\n",
    "with open(\"data.zip\", \"wb\") as file:\n",
    "    file.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed4e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip data\n",
    "\n",
    "#zipfile.ZipFile 用于读写 ZIP 文件的类\n",
    "import zipfile\n",
    "with zipfile.ZipFile('data.zip') as zip:\n",
    "    zip.extractall('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b7b8be",
   "metadata": {},
   "source": [
    "## 2.2 Load data\n",
    "\n",
    "- training data 有2个文件，每个文件包含 1700 条抱怨/非抱怨推文。数据中的每条推文都至少包含一个航空公司的标签\n",
    "- 只使用文本数据进行分类，所以我们将删除不重要的列，只保留id,tweet和label列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3506d722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>46521</td>\n",
       "      <td>@lufthansa Pilots in strike, now I have to fly...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>93315</td>\n",
       "      <td>@DeltaAssist @Delta Are any other flights depa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>51328</td>\n",
       "      <td>@CortJstr You're flying @VirginAmerica aren't ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>70414</td>\n",
       "      <td>@SouthwestAir My friends are having the #worst...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>3372</td>\n",
       "      <td>I appreciate humor injected into @Delta &amp;amp; ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              tweet  label\n",
       "360   46521  @lufthansa Pilots in strike, now I have to fly...      0\n",
       "1045  93315  @DeltaAssist @Delta Are any other flights depa...      0\n",
       "2289  51328  @CortJstr You're flying @VirginAmerica aren't ...      1\n",
       "1383  70414  @SouthwestAir My friends are having the #worst...      0\n",
       "1727   3372  I appreciate humor injected into @Delta &amp; ...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Load data and set labels\n",
    "data_complaint = pd.read_csv('data/complaint1700.csv')\n",
    "data_complaint['label'] = 0\n",
    "data_non_complaint = pd.read_csv('data/noncomplaint1700.csv')\n",
    "data_non_complaint['label'] = 1\n",
    "\n",
    "# Concatenate complaining and non-complaining data\n",
    "# pd.concat 沿特定轴连接 pandas 对象 \n",
    "# dataframe.reset_index() 重置 DataFrame 的索引, drop = True 避免将旧索引添加为列\n",
    "data = pd.concat([data_complaint, data_non_complaint], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Drop 'airline' column\n",
    "# dataframe.drop()删除行或者列，\n",
    "data.drop(['airline'], inplace=True, axis=1)\n",
    "\n",
    "# Display 5 random samples\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ff4735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.tweet.values\n",
    "y = data.label.values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1165d33",
   "metadata": {},
   "source": [
    "## 2.3 Load Test Data\n",
    "\n",
    "测试数据包含 4555 个没有标签的示例。大约 300 个示例是非抱怨的推文。我们的任务是识别它们id并手动检查我们的结果是否正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "248d4571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>16307</td>\n",
       "      <td>Another crappy experience with @AmericanAir. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>91392</td>\n",
       "      <td>Never #fly @AmericanAir as they will take your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>171201</td>\n",
       "      <td>@united this is disgusting behaviour. #Islamop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3348</th>\n",
       "      <td>126726</td>\n",
       "      <td>This shit has you put in all your info on @Ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>19872</td>\n",
       "      <td>@JetBlue truly disappointed in the customer se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4409</th>\n",
       "      <td>167045</td>\n",
       "      <td>Stuck at @flyLAXairport due to air traffic con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190</th>\n",
       "      <td>84486</td>\n",
       "      <td>@MaratRyndin @JetBlue  Bad Ass pilots! My favo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>15322</td>\n",
       "      <td>@AmericanAir I had multiple flights yesterday....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>80917</td>\n",
       "      <td>@DeltaAssist as a student who plans to be a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>94495</td>\n",
       "      <td>@united costumer service sucks! Waiting for a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet\n",
       "414    16307  Another crappy experience with @AmericanAir. T...\n",
       "2389   91392  Never #fly @AmericanAir as they will take your...\n",
       "4497  171201  @united this is disgusting behaviour. #Islamop...\n",
       "3348  126726  This shit has you put in all your info on @Ame...\n",
       "507    19872  @JetBlue truly disappointed in the customer se...\n",
       "4409  167045  Stuck at @flyLAXairport due to air traffic con...\n",
       "2190   84486  @MaratRyndin @JetBlue  Bad Ass pilots! My favo...\n",
       "383    15322  @AmericanAir I had multiple flights yesterday....\n",
       "2099   80917  @DeltaAssist as a student who plans to be a li...\n",
       "2472   94495  @united costumer service sucks! Waiting for a ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test data\n",
    "test_data = pd.read_csv('data/test_data.csv')\n",
    "\n",
    "# Keep important columns\n",
    "test_data = test_data[['id', 'tweet']]\n",
    "\n",
    "# Display 5 samples from the test data\n",
    "test_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed9d176",
   "metadata": {},
   "source": [
    "# 3. Set up GPU for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d3e43ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aab16f",
   "metadata": {},
   "source": [
    "# 4.设定基线 TF-IDF + 朴素贝叶斯分类器\n",
    "\n",
    "## 4.1 预处理\n",
    "\n",
    "- 词袋，不考虑语法和词序。因此，我们将要删除对句子含义贡献不大的停用词、标点符号和字符\n",
    "- 例子：\n",
    "现在数据集里有两个句子：\n",
    "\n",
    "句子1：The cat sat on the hat\n",
    "\n",
    "句子2：The dog ate the cat and the hat\n",
    "\n",
    "所以，我们的词库就是{ the, cat, sat, on, hat, dog, ate, and }，Bag of words就以词库的单词数为特征维数，对应元素就是该位置单词在该句子中出现的次数，比如\n",
    "\n",
    "句子1：{ 2, 1, 1, 1, 1, 0, 0, 0 }\n",
    "\n",
    "句子2：{ 3, 1, 0, 0, 1, 1, 1, 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e2457c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/WYF/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28214d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(s):\n",
    "    \"\"\"\n",
    "    - Lowercase the sentence\n",
    "    - Change \"'t\" to \"not\"\n",
    "    - Remove \"@name\"\n",
    "    - Isolate and remove punctuations except \"?\"\n",
    "    - Remove other special characters\n",
    "    - Remove stop words except \"not\" and \"can\"\n",
    "    - Remove trailing whitespace\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    # Change 't to 'not'\n",
    "    s = re.sub(r\"\\'t\", \" not\", s)\n",
    "    # Remove @name\n",
    "    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n",
    "    # Isolate and remove punctuations except '?'\n",
    "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
    "    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
    "    # Remove some special characters\n",
    "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
    "    # Remove stopwords except 'not' and 'can'\n",
    "    s = \" \".join([word for word in s.split()\n",
    "                  if word not in stopwords.words('english')\n",
    "                  or word in ['not', 'can']])\n",
    "    # Remove trailing whitespace\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b0656a",
   "metadata": {},
   "source": [
    "## 4.2 TF-IDF Vectorizer\n",
    "\n",
    "- 使用TfidVectorizer\n",
    "- 先定义tfid= TFidVectorizer\n",
    "- 然后用 tfid.fit_transform(数据)进行求TFIDF\n",
    "\n",
    "### n-gram模型\n",
    "\n",
    "基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列\n",
    "\n",
    "基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "244bcc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 s, sys: 2.49 s, total: 13.2 s\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Preprocess text\n",
    "X_train_preprocessed = np.array([text_preprocessing(text) for text in X_train])\n",
    "X_val_preprocessed = np.array([text_preprocessing(text) for text in X_val])\n",
    "\n",
    "# Calculate TF-IDF\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                         binary=True,\n",
    "                         smooth_idf=False)\n",
    "X_train_tfidf = tf_idf.fit_transform(X_train_preprocessed) # fit_transform学习词汇和 idf，返回文档术语矩阵。\n",
    "X_val_tfidf = tf_idf.transform(X_val_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a45ebe68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x54478 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 36 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b5cf050",
   "metadata": {},
   "source": [
    "## 4.3 Train Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a62a56cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross-validation and AUC score to tune hyperparameters of our model. \n",
    "# The function get_auc_CV 返回交叉验证的平均 AUC 分数.\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "def get_auc_CV(model):\n",
    "    \"\"\"\n",
    "    Return the average AUC score from cross-validation.\n",
    "    \"\"\"\n",
    "    # Set KFold to shuffle data before the split\n",
    "    kf = StratifiedKFold(5, shuffle=True, random_state=1) # shuffle = True, 把样本打散\n",
    "\n",
    "    # Get AUC scores\n",
    "    auc = cross_val_score(\n",
    "        model, X_train_tfidf, y_train, scoring=\"roc_auc\", cv=5)\n",
    "\n",
    "    return auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3134e82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find alpha\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f170851",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_5/nbn8qmmj3bx_4cqrbly24rg00000gn/T/ipykernel_50421/3988216162.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m res = pd.Series([get_auc_CV(MultinomialNB(i)) for i in np.arange(1, 10, 0.1)],\n\u001b[0m\u001b[1;32m      2\u001b[0m                 index=np.arange(1, 10, 0.1))\n",
      "\u001b[0;32m/var/folders/_5/nbn8qmmj3bx_4cqrbly24rg00000gn/T/ipykernel_50421/3988216162.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m res = pd.Series([get_auc_CV(MultinomialNB(i)) for i in np.arange(1, 10, 0.1)],\n\u001b[0m\u001b[1;32m      2\u001b[0m                 index=np.arange(1, 10, 0.1))\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "res = pd.Series([get_auc_CV(MultinomialNB(i)) for i in np.arange(1, 10, 0.1)],\n",
    "                index=np.arange(1, 10, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a78450",
   "metadata": {},
   "source": [
    "### 评估\n",
    "计算模型在验证集上的准确率和 AUC 分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3611b542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0479389a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8411\n",
      "Accuracy: 75.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAA0yklEQVR4nO3deXgUVdbA4d8h7LuCO6I4IAIKihFxAVRGxRUdXHBcRkcGUEFFEXHFD/cdVBYDjrjCKCOIioIoiyOjiIoQVhnWCCqbyCJKwvn+uNWmaZNOkaS6ejnv8/ST7q7qqtOVpE/fe6vOFVXFGGOMKU6FsAMwxhiT3CxRGGOMicsShTHGmLgsURhjjInLEoUxxpi4LFEYY4yJyxKF2SMiMl9ETgk7jmQhIneKyMiQ9j1KRB4IY9/lTUQuF5HJpXyt/U0GzBJFChORFSLyi4hsFZHvvQ+OmkHuU1VbqOq0IPcRISJVRORhEVnlvc9vReQ2EZFE7L+IeE4Rkbzo51T1IVXtFtD+RERuFJFcEdkmInki8qaIHBXE/kpLRO4TkVfLsg1VfU1Vz/Cxrz8kx0T+TWYqSxSp7zxVrQkcDRwD3BFuOHtORCoWs+hNoCNwNlALuBLoDgwOIAYRkWT7fxgM3ATcCOwNHA6MB84p7x3F+R0ELsx9G59U1W4pegNWAH+OevwY8F7U47bATOAn4BvglKhlewMvAmuATcD4qGXnAnO8180EWsbuEzgQ+AXYO2rZMcB6oJL3+O/AQm/7k4BDotZV4AbgW2B5Ee+tI7ADODjm+eOBAqCx93ga8DAwC9gMvB0TU7xjMA14EPjUey+NgWu8mLcAy4Ae3ro1vHV2AVu924HAfcCr3jqHeu/rb8Aq71jcFbW/asBL3vFYCPQD8or53Tbx3mebOL//UcAQ4D0v3s+BP0UtHwysBn4GvgTaRS27DxgLvOot7wa0Af7rHau1wHNA5ajXtAA+BDYCPwB3Ap2A34Cd3jH5xlu3DvCCt53vgAeALG/Z1d4xf9rb1gPec//xlou37EfvdzoXOBL3JWGnt7+twDux/wdAlhfX/7xj8iUxf0N2K8VnTdgB2K0Mv7zd/0EaAPOAwd7jg4ANuG/jFYDTvcf7eMvfA/4F7AVUAjp4z7f2/kGP9/7p/ubtp0oR+/wY+EdUPI8Dw737FwBLgWZAReBuYGbUuup96OwNVCvivT0CTC/mfa+k8AN8mvdBdCTuw/zfFH5wl3QMpuE+0Ft4MVbCfVv/k/dh1QHYDrT21j+FmA92ik4UI3BJoRXwK9As+j15x7wB7gOwuETRE1hZwu9/FO6Dto0X/2vAmKjlVwD1vGW3At8DVaPi3un9nip48R6LS6wVvfeyELjZW78W7kP/VqCq9/j42GMQte/xwPPe72RfXCKP/M6uBvKB3t6+qrF7ojgT9wFf1/s9NAMOiHrPD8T5P7gN93/Q1HttK6Be2P+rqX4LPQC7leGX5/5BtuK+OSnwEVDXW3Y78ErM+pNwH/wH4L4Z71XENocB98c8t5jCRBL9T9kN+Ni7L7hvr+29x+8D10ZtowLuQ/cQ77ECp8V5byOjP/Riln2G900d92H/SNSy5rhvnFnxjkHUaweWcIzHAzd590/BX6JoELV8FtDVu78MODNqWbfY7UUtuwv4rITYRgEjox6fDSyKs/4moFVU3DNK2P7NwDjv/mXA18Ws9/sx8B7vh0uQ1aKeuwyY6t2/GlgVs42rKUwUpwFLcEmrQhHvOV6iWAx0Luv/lt12vyVbn6zZcxeoai3ch9gRQH3v+UOAi0Xkp8gNOBmXJA4GNqrqpiK2dwhwa8zrDsZ1s8QaC5wgIgcC7XEfkp9EbWdw1DY24pLJQVGvXx3nfa33Yi3KAd7yorazEtcyqE/8Y1BkDCJyloh8JiIbvfXPpvCY+vV91P3tQOQEgwNj9hfv/W+g+PfvZ1+IyK0islBENnvvpQ67v5fY9364iLzrnRjxM/BQ1PoH47pz/DgE9ztYG3Xcn8e1LIrcdzRV/RjX7TUE+EFEckSkts9970mcxidLFGlCVafjvm094T21Gvdtum7UrYaqPuIt21tE6haxqdXAgzGvq66qo4vY50/AZOAS4K/AaPW+1nnb6RGznWqqOjN6E3He0hTgeBE5OPpJEWmD+zD4OOrp6HUa4rpU1pdwDP4Qg4hUwXVdPQHsp6p1gYm4BFdSvH6sxXU5FRV3rI+ABiKSXZodiUg7XIvqElzLsS6uvz/6jLHY9zMMWAQ0UdXauL7+yPqrcV1yRYndzmpci6J+1HGvraot4rxm9w2qPqOqx+K6BQ/HdSmV+LoS4jSlZIkivQwCTheRo3GDlOeJyJkikiUiVb3TOxuo6lpc19BQEdlLRCqJSHtvGyOAniJyvHcmUA0ROUdEahWzz9eBq4Au3v2I4cAdItICQETqiMjFft+Iqk7BfVj+W0RaeO+hLa4ffpiqfhu1+hUi0lxEqgMDgbGqWhDvGBSz28pAFWAdkC8iZwHRp2z+ANQTkTp+30eMN3DHZC8ROQjoVdyK3vsbCoz2Yq7sxd9VRPr72Fct3DjAOqCiiNwLlPStvBZuYHuriBwBXBe17F1gfxG52TttuZaIHO8t+wE4NHLWmPf3NRl4UkRqi0gFEfmTiHTwETcicpz391cJ2IY7qaEgal+HxXn5SOB+EWni/f22FJF6fvZrimeJIo2o6jrgZeAeVV0NdMZ9K1yH+6Z1G4W/8ytx37wX4Qavb/a2MRv4B67pvwk3IH11nN1OwJ2h84OqfhMVyzjgUWCM142RC5y1h2+pCzAV+AA3FvMq7kya3jHrvYJrTX2PG2i90YuhpGOwG1Xd4r32Ddx7/6v3/iLLFwGjgWVel0pR3XHxDATygOW4FtNY3Dfv4txIYRfMT7gulQuBd3zsaxLuy8ASXHfcDuJ3dQH0xb3nLbgvDP+KLPCOzenAebjj/C1wqrf4Te/nBhH5yrt/FS7xLsAdy7H460oDl9BGeK9bieuGi7SUXwCae8d/fBGvfQr3+5uMS3ov4AbLTRlIYU+BMalHRKbhBlJDuTq6LETkOtxAt69v2saExVoUxiSIiBwgIid5XTFNcaeajgs7LmNKEliiEJF/isiPIpJbzHIRkWdEZKmIzBWR1kHFYkySqIw7+2cLbjD+bdw4hDFJLbCuJ29wdCvwsqoeWcTys3F9zWfjLu4arKrHx65njDEmXIG1KFR1Bu7c+eJ0xiURVdXPgLoi4newyxhjTIKEWYzrIHY/CyPPe25t7Ioi0h1X54UaNWoce8QRRyQkQGOMKavFi+GXX6BaSOde1fttLXv/9j1fs2u9qu5Tmm2EmSiKKhVdZD+YquYAOQDZ2dk6e/bsIOMyxoQkJwdef73k9VJJVhacfDJMm5bgHauCCEyYAJMnI0OGrCztpsJMFHnsfmVqA1wlU2NMBigqKUyf7n52SKMTho8+Gv761wTucNMm6NsXDjsM7roLzj/f3YYMKfUmw0wUE4BeIjIGN5i92bui0xiTwvy2CopKCh06uA/V7t2DiS3tjRsH118P69bB3XeX22YDSxQiMhpXqK6+uFnBBuAKhaGqw3E1dM7GXfm7HTcPgDEmxb3+OsyZ475Jx2NJoRz98AP07g1vvukO/HvvQevyu+IgsEShqpeVsFxxE9cYY1KA35ZCJEkkvE8+k61e7ZLDgw/CbbdBpUrlunmbgtCYDFHWgWK/4wcJ75PPVCtXwjvvQK9ekJ0Nq1ZBvWDqH1qiMCYN+EkCZR0otq6iJLFrFwwbBv29IsJdusABBwSWJMAShTEpKTYx+EkC9kGfBhYvhm7d4D//gTPPhOefd0kiYJYojElBsQPGlgQywPbt7oKMggIYNQquuspdJ5EAliiMCVFpxw1swDiDLFkCTZpA9erwyivuF7///gkNwRKFMQEqKRGUdtzABowzwI4dcP/98OijrgVxxRXQqVMooViiMCZAJV1TYF1GpkiffgrXXuvGJK65Bs45J9RwLFEYEzDrIjJ75P77YcAAaNgQJk2CM84o+TUBs0RhTBnF617yc4WyMUBhEb+jj3ZXWT/4INSsGXZUgE2FakyZRbqXimJjCaZEGzfC3/4GDzzgHp93HgwenDRJAqxFYYxvxbUc7AwkU2pjx8INN7hkcc89YUdTLEsUxsQoLiEUd4aStRrMHlu71pXeeOstOPZYmDwZWrUKO6piWaIwJkZxZyrZGUqm3KxZ4waqH30UbrkFKib3R3FyR2dMAsS2IKwryQRixQpXxK93b9eKWL0a9tor7Kh8sURh0kZpr3KO7VKyriRTrgoK3Oxyd94JFSrAxRe7K6tTJEmAJQqTJnJyoEcPd39Pr3K2LiUTmIULXRG/mTPdVdXPP5/w8hvlwRKFSQuRlsTzz9sHvkkS27dD+/auLPjLL7sSHAkq4lfeLFGYlFBSt9KcOa5lYEnChG7RImja1BXxe+01dzbTfvuFHVWZ2AV3JiXEu6gNbFzBJIFffoHbb4cWLVyCAFd+I8WTBFiLwiS5SEvCzkQySW3GDDcW8e237ue554YdUbmyFoVJatFJwloMJin93/+5fs/8fJgyBUaMgLp1w46qXFmLwiQla0mYpBcp4pedDX36uKqvNWqEHVUgLFGYpBJJENHXNlhLwiSV9etdYmjSBO69180VEfJ8EUGzRGGSQnEJws5iMklDFd5809Vo2rTJzRmRISxRmFDEnu5qCcIktTVr4Prr4e23XVfTlCnQsmXYUSWMJQoTitjCe5YgTFL7/nv4+GN4/HG4+eakL+JX3jLr3ZqkYoPUJqktWwYTJrjE0Lo1rFqVdmcz+WWJwiREcRVajUk6BQXwzDNw111QqRJ07erqM2VokgC7jsIkSOyV1XZdhElK8+fDSSe5OSJOO809TsEifuXNWhQmYayrySS17dvdYJmI+2bTtWvKFvErb5YoTLnwU7TPuppMUlqwAJo1c0X8xoxxRfz22SfsqJKKJQpTrD2ZCKi4+aQjrKvJJJ3t2921EE89BaNGwZVXwp//HHZUSckShSlWcXNHF8VObzUpZdo0+Mc/YOlSN+PV+eeHHVFSs0Rh4rJxBZN2BgyAgQPhT39y10acemrYESU9O+vJGJMZVN3PNm3g1lth7lxLEj4FmihEpJOILBaRpSLSv4jldUTkHRH5RkTmi8g1QcZjjMlA69a5ftGBA93jc86BJ55wg9fGl8AShYhkAUOAs4DmwGUi0jxmtRuABaraCjgFeFJEKgcVkzEmg6i6gbZmzWDsWKhsHy2lFeQYRRtgqaouAxCRMUBnYEHUOgrUEhEBagIbgfwAYzJx2NXTJm3k5cF118G778Lxx8MLL7gpSk2pBNn1dBCwOupxnvdctOeAZsAaYB5wk6ruit2QiHQXkdkiMnvdunVBxZvx7OppkzbWrXPTkz71FHz6qSWJMgqyRVHUJY0a8/hMYA5wGvAn4EMR+URVf97tRao5QA5AdnZ27DZMKRXXgrCznExKWroU3nnHTSp0zDGwejXUrh12VGkhyBZFHnBw1OMGuJZDtGuAt9RZCiwHjggwJhPFWhAmLeTnu8Hpo45y81f/8IN73pJEuQmyRfEF0EREGgHfAV2B2I+hVUBH4BMR2Q9oCiwLMKaM4reshrUgTMqaNw+uvRa++MJdNDd0KOy3X9hRpZ3AEoWq5otIL2ASkAX8U1Xni0hPb/lw4H5glIjMw3VV3a6q64OKKdOUdGW1tSBMStu+3V0HUaGCq9F0ySVWxC8ggV6ZraoTgYkxzw2Pur8GOCPIGDKdtRhM2snNdYPT1avDv/7livjVrx92VGnNrsw2xqSGbdvcPBEtW8Krr7rnOna0JJEAlijSUE4OnHLK7gPVxqS0jz5yg9VPP+2uj+jcOeyIMoolijQUPTZhYxAm5d1zjyv/XbGiq2c/ZIid0ZRgVj02TdnYhEl5u3a5geoTT4R+/eC++6BatbCjykiWKNKAld4waeXHH+HGG6FpU3ddxFlnuZsJjXU9pbDIWESPHoUzzIF1OZkUpeoGqZs1g3HjrLprErEWRQqLjEXY7HIm5a1eDT17wsSJcMIJMHIkNI8tNm3CYokixdlYhEkLGza44n2DB8MNN0BWVtgRmSjW9ZSC7PRXkxaWLHE1msB941m92o1NWJJIOpYoUpCd/mpSWn4+PPqou3DuwQcLi/jVqhVuXKZY1vWUoqzLyaSkb76Bv/8dvvoKLrzQXRNhRfySniUKY0xibN/uSm5UrOimJu3SJeyIjE+WKIwxwZo715XfqF4d3nzTFfHbe++wozJ7wMYojDHB2LoVbrrJ9ZO+8op77tRTLUmkIGtRGGPK34cfugt7VqyAXr3ceIRJWdaiSCF2WqxJCXfdBWecAVWqwCefwLPP2hlNKc53ohCRGkEGYkpmp8WapLZrl/t58slwxx3uj/Xkk0MNyZSPErueROREYCRQE2goIq2AHqp6fdDBmT+y02JN0vn+e9e91Lw5DBxoRfzSkJ8WxdPAmcAGAFX9BmgfZFCmUKS7ybqcTNJRhVGjXIJ4912bIyKN+ep6UtXVMU8VBBCLKUKkuwmsy8kkkZUroVMnuOYaN3/1N99A375hR2UC4uesp9Ve95OKSGXgRmBhsGGZaNbdZJLOTz/BF1/Ac8+5qUkr2Hkx6cxPougJDAYOAvKAyYCNTwTEJiEySWvxYpgwAW67zV00t2oV1KwZdlQmAfx8DWiqqper6n6quq+qXgE0CzqwTBXd1QTW3WSSwM6d8PDDLjk88oibgQ4sSWQQPy2KZ4HWPp4z5cS6mkzS+PpruPZa9/Oii1xX0777hh2VSbBiE4WInACcCOwjIrdELaoNWMH4MortYoqwriaTNLZvh9NPh0qV4N//hr/8JeyITEjidT1Vxl07URGoFXX7Gbgo+NDSW2wXU4R1NZnQff21O/W1enVX5XXBAksSGa7YFoWqTgemi8goVV2ZwJgyhnUxmaSyZYu7onrIEHjpJbjqKncBj8l4fsYotovI40ALoGrkSVU9LbCo0liky8m6mExS+eAD6NHDTUd6003WgjC78XPW02vAIqAR8H/ACuCLAGNKa1avySSdO+5wJTdq1IBPP4VBg+yMJrMbPy2Keqr6gojcFNUdNT3owNKZdTmZpFBQAFlZrnupYkW4+25X8dWYGH4SxU7v51oROQdYAzQILiRjTKDWroUbbnClN+6/H848092MKYafrqcHRKQOcCvQF1dJ9uYggzLGBEAVXnzRFfF7/33Ya6+wIzIposQWhaq+693dDJwKICInBRlUOrGSHCYprFgB//gHTJkC7drByJFw+OFhR2VSRLEtChHJEpHLRKSviBzpPXeuiMwEnktYhCnOSnKYpLB5M3z1FQwd6gbILEmYPRCvRfECcDAwC3hGRFYCJwD9VXW8n42LSCdcQcEsYKSqPlLEOqcAg4BKwHpV7eA//NRgg9cmFAsWuCJ+/fsXFvGrYRNVmj0XL1FkAy1VdZeIVAXWA41V9Xs/GxaRLGAIcDqu6uwXIjJBVRdErVMXGAp0UtVVImJFZIwpq99+g8cecwPVtWrB3//u6jNZkjClFG8w+zdV3QWgqjuAJX6ThKcNsFRVl6nqb8AYoHPMOn8F3lLVVd5+ftyD7RtjYs2eDccdB/fc4y6aW7DAiviZMouXKI4QkbnebV7U43kiMtfHtg8ComfGy/Oei3Y4sJeITBORL0XkqqI2JCLdRWS2iMxet26dj12HLzKFqU1fahJm2zZ3muv69fD22zB6tCUJUy7idT2Vdc4JKeI5LWL/xwIdgWrAf0XkM1VdstuLVHOAHIDs7OzYbSQluwLbJMxXX7k/tBo1YNw4aNkS6tYNOyqTRuIVBSxrIcA83GB4RAPcxXqx66xX1W3ANhGZAbQClpAGbBDbBOrnn91A9bBhhUX82rcPOyqThoKc6PYLoImINPLm2u4KTIhZ522gnYhUFJHqwPGk6Hzcka6myM26nEygJk50V1Y//zzccgt06RJ2RCaNBZYoVDUf6AVMwn34v6Gq80Wkp4j09NZZCHwAzMWdhjtSVXODiilIdr2ESZjbb4dzzoHatWHmTHjySTujyQTKT60nRKQa0FBVF+/JxlV1IjAx5rnhMY8fBx7fk+0mK+tqMoFRhV27XBG/jh2halW4804r4mcSosQWhYicB8zBffNHRI4WkdgupIxlZzeZwH33HVxwAQwY4B6fcQb83/9ZkjAJ46fr6T7cNRE/AajqHODQoAJKFZEE0aMHTJ9uXU0mAKowYoQr4jd5MtSvH3ZEJkP56XrKV9XNIkWd7Zq5ImMSHTq4BNG9e9gRmbSyfDlcey1Mneq+kYwYAY0bhx2VyVB+EkWuiPwVyBKRJsCNwMxgw0oNNiZhArN1K8yd685q6tYNKgR5gqIx8fn56+uNmy/7V+B1XLnxmwOMyZjMlJsLDz3k7h91lCvi1727JQkTOj9/gU1V9S5VPc673e3VfspINnhtyt1vv7nB6dat4emn4Uev5Fn16uHGZYzHT9fTUyJyAPAmMEZV5wccU1KKTEA03ZstPDI2YUyZfPGFq+6am+v+oAYNgn32CTsqY3bjZ4a7U0Vkf+ASIEdEagP/UtUHAo8uidjgtSl327ZBp05QrZqbN+K888KOyJgi+brgzisv/oyITAX6AfcCGZUowAavTTmZPdt1M9Wo4aq8HnUU1KkTdlTGFMvPBXfNROQ+EcnFTYE6E1fgLyPYmIQpN5s3uwtvjjsOXn3VPXfyyZYkTNLz06J4ERgNnKGqsdVf056VCzfl4p13oGdP+P576NsXLroo7IiM8c3PGEXbRASSzKzLyZTJbbfBE0+4Lqbx412LwpgUUmyiEJE3VPUSb3a76MmCBFBVbRl4dMakKlUoKICKFV1tptq1XdXXypXDjsyYPRavRXGT9/PcRARiTNrIy4PrrnMzzT34IJx+ursZk6KKHcxW1bXe3etVdWX0Dbg+MeEZk0J27XIlN5o3h48/hv33DzsiY8qFnyuzi/oqdFZ5B2JMSlu2DE47zQ1Yt2kD8+ZB795hR2VMuYg3RnEdruVwmIjMjVpUC/g06MCMSSnbtsGCBTBypLvS2qotmzQSb4zideB94GGgf9TzW1R1Y6BRGZMK5s1zF8zdfbc7o2nlSneVtTFpJl7Xk6rqCuAGYEvUDRHZO/jQjElSv/4K997rrq5+5pnCIn6WJEyaKqlFcS7wJe702Oi2tAKHBRiXMcnps8/chEILFsCVV7pqr/XqhR2VMYEqNlGo6rnez0aJC8eYJLZtG5xzjqvRNHEinGXndJjM4KfW00kiUsO7f4WIPCUiDYMPzZgk8fnn7tTXGjVcKY758y1JmIzi5/TYYcB2EWmFqxy7Engl0KiSgBUDNPz0k5uGtG3bwiJ+J54ItWqFGpYxieYnUeSrqgKdgcGqOhh3imxas2KAGW78eHfh3KhRrvTGxReHHZExofFTPXaLiNwBXAm0E5EsoFKwYSUHKwaYoW65xQ1St2rlupqOPTbsiIwJlZ9EcSnwV+Dvqvq9Nz7xeLBhGZNg0UX8zj7bncnUrx9UyojvRMbEVWLXkze73WtAHRE5F9ihqi8HHpkxibJqlTubacAA9/jPf4a77rIkYYzHz1lPlwCzgItx82Z/LiI264pJfbt2wdCh0KIFTJ8OBx4YdkTGJCU/XU93Acep6o8AIrIPMAUYG2RgxgRq6VJXk+mTT1wJ8JwcOPTQsKMyJin5SRQVIknCswF/Z0sZk7x27IAlS+DFF+Fvf7MifsbE4SdRfCAik3DzZoMb3J4YXEjGBGTOHFfEb8AAOPJIWLECqlYNOypjkp6fwezbgOeBlkArIEdVbw86sDDl5Lgua5Mmduxwg9PZ2TBsWGERP0sSxvgSbz6KJsATwJ+AeUBfVf0uUYGF6fXX3U+70C4NzJzpivgtWuS6mJ56Cva24sfG7Il4LYp/Au8CXXAVZJ9NSERJokMH6N497ChMmWzbBuedB9u3wwcfuKusLUkYs8fiJYpaqjpCVRer6hPAoQmKKTRW3ylN/Pe/hUX83n0XcnPhzDPDjsqYlBUvUVQVkWNEpLWItAaqxTwukYh0EpHFIrJURPrHWe84ESkI+/oMq++U4jZtcqe8nngivOLVrTzhBCviZ0wZxTvraS3wVNTj76MeK3BavA17NaGGAKcDecAXIjJBVRcUsd6jwKQ9Cz0YVt8pRb31FtxwA6xbB3fcAZdeGnZExqSNeBMXnVrGbbcBlqrqMgARGYOrQLsgZr3ewL+B48q4vz2Wk1M4cA2FrQmTYvr0gUGD3C9v4kQ45piwIzImrQR54dxBwOqox3nec78TkYOAC4Hh8TYkIt1FZLaIzF63bl25BRjpaoqwLqcUogr5+e7+uefCQw/BrFmWJIwJgJ8L7kqrqEtdNebxIOB2VS2QOFfGqmoOkAOQnZ0du40ysa6mFLRiBfToAa1bw8MPQ8eO7maMCUSQLYo84OCoxw2ANTHrZANjRGQFcBEwVEQuCDAmk8p27YJnn3VXVc+cCYccEnZExmSEElsU4r7qXw4cpqoDvfko9lfVWSW89AugiYg0Ar4DuuLmtfidqjaK2s8o4F1VHb9H78Bkhm+/hWuugU8/hU6dYPhwSxTGJIifFsVQ4ATgMu/xFtzZTHGpaj7QC3c200LgDVWdLyI9RaRnKeM1meq33+B//4OXX3YD1pYkjEkYP2MUx6tqaxH5GkBVN4lIZT8bV9WJxBQQVNUiB65V9Wo/2zQZ5OuvXRG/++5zc0asWAFVqoQdlTEZx0+LYqd3rYPC7/NR7Ao0KpPZduxw10Icdxw8/7y7NgIsSRgTEj+J4hlgHLCviDwI/Ad4KNCoTOb6z3+gVSt45BG46ipYsAD22SfsqIzJaCV2PanqayLyJdARd8rrBaq6MPDIAhS50M4usEsyW7dC585QuzZMnuxmnjPGhM7PWU8Nge3AO9HPqeqqIAMLktV0SjL/+Y+rz1SzJrz3njv9tWbNsKMyxnj8DGa/hxufEKAq0AhYDLQIMK7A2YV2SWDDBld+45VXXAnwv/0N2rYNOypjTAw/XU9HRT/2Ksf2CCwik/5UYexY6NULNm6Ee+6Brl3DjsoYU4w9LuGhql+JSMIL+Jk00qcPDB4Mxx7rxiJatQo7ImNMHH7GKG6JelgBaA2UX2U+kxkiRfwqVYLzz4cDD4RbboGKQZYbM8aUBz+nx9aKulXBjVl0DjIok2aWL4czznBdTACnnQb9+lmSMCZFxP1P9S60q6mqtyUoHpNOCgrguefgzjshKwsuvjjsiIwxpVBsi0JEKqpqAa6rKS3YnNgJtGQJtGsHN98MHTrA/PnQvXvYURljSiFei2IWLknMEZEJwJvAtshCVX0r4NjKnV0/kUD5+bByJbz6qjvYceYbMcYkNz+dxHsDG3BzZEeup1Ag5RIF2PUTgZo92xXxu/9+aN4cli2z+kzGpIF4iWJf74ynXAoTRES5zjJnUtwvv8CAAfDkk7D//nDjja4+kyUJY9JCvLOesoCa3q1W1P3ILWXY2ESApk+Hli3h8cfh2mvdWIQV8TMmrcRrUaxV1YEJiyRANjYRkK1b4S9/gbp14aOP3Gmvxpi0Ey9RpNXoo41NlKNPPoGTTnKF+95/300qVKNG2FEZYwISr+upY8KiMKlh/Xq44gpo394V8gNo08aShDFprtgWhapuTGQgJompwhtvQO/esGmTG7i2In7GZAyroWBKdtNN8OyzbmrSjz6Co44q+TXGmLRhicIUTRV27oTKleHCC+GQQ9xV1llZYUdmjEkwP0UBTab53/+gY0e4+273+NRT4dZbLUkYk6EsUZhCBQXw1FOua+nLL6Fp07AjMsYkAet6Ms6iRW4q0lmz4LzzYNgwOOigsKMyxiQBSxTG2bUL1qyB0aPh0kutiJ8x5neWKDLZrFmuiN+DD7oifv/7nxu8NsaYKDZGkYm2b4e+feGEE+Cll2CdN7OtJQljTBEsUWSaqVPdYPWTT8I//mFF/IwxJUrrRGFVY2Ns3eqmIxVxCWP4cKhTJ+yojDFJLq0ThVWN9Uyb5garI0X85s51GdQYY3xI60QBhVVjM3K65nXr4LLL3AVzr77qnjvuOKhePdy4jDEpxc56Skeq7jTXG2+ELVvc1KRWxM8YU0qWKNJR794wZAi0bQsvvOBOfTXGmFKyRJEudu2C/Hx3iutFF0Hjxi5hWH0mY0wZBTpGISKdRGSxiCwVkf5FLL9cROZ6t5ki0irIeNLWt9+6aUjvuss9PuUUq/RqjCk3gSUKEckChgBnAc2By0Qktg9kOdBBVVsC9wM5QcWTlvLz4YknoGVLd3pXs2ZhR2SMSUNBdj21AZaq6jIAERkDdAYWRFZQ1ZlR638GNAgwnvSycCFcdRXMng2dO8PQoXDggWFHZYxJQ0F2PR0ErI56nOc9V5xrgfeLWiAi3UVktojMXhcpNxFHxlxo98MP8K9/wbhxliSMMYEJMlEUVX5Ui1xR5FRcori9qOWqmqOq2aqavY+PchNpe6HdZ5/BHXe4+82auSJ+l1xilV6NMYEKMlHkAQdHPW4ArIldSURaAiOBzqq6obx2nlYX2m3bBn36wIknwmuvFRbxq1Qp3LiMMRkhyETxBdBERBqJSGWgKzAhegURaQi8BVypqksCjCV1TZkCRx4JgwbB9ddbET9jTMIFNpitqvki0guYBGQB/1TV+SLS01s+HLgXqAcMFdd9kq+q2aXdZ07O7t1OKW/rVndF9d57w4wZ0K5d2BEZYzJQoBfcqepEYGLMc8Oj7ncDupXX/tJmbOLjj6FDB1fEb9Ikd2V1tWphR2WMyVBpVxQwpccmfvjBDU537FhYxO/YYy1JGGNClXaJIiWpwiuvuJZDZGrSlG4SGWPSidV6SgY33ADDhrmpSV94wa6wNsYklZROFJHB64iUGsTetQt27oQqVeDSS11yuP56q89kjEk6Kd31FBm8jkiZQezFi91gdaSIX4cOVunVGJO0UrpFAYWD1ylh50548km47z43QN2t3E74MsaYwKR8okgZ8+fDlVfC11/DX/7iJhbaf/+wozLGmBJZokiUrCzYuBHGjoUuXcKOxhhjfEvpMYqkN3Mm3O7VOTziCFi61JKEMSblWKIIwtatcOONcPLJrgz4+vXu+YrWgDPGpB5LFOVt8mRXxO+556BXL8jNhfr1w47KGGNKLSUTRdJOTLR1K1x+OVStCp98As884+o1GWNMCkvJRJF0xf8+/BAKClxSmDzZBXfSSWFHZYwx5SIlEwUkSfG/tWvd4PQZZ7gJhQCOOca1KIwxJk2kXKJYvDgJupxUYdQoV8TvvffgkUeSpGljjDHlL+VOw/nlF3cyUaify9ddB88/7wIZORKaNg0xGGOS186dO8nLy2PHjh1hh5IxqlatSoMGDahUjlMli6qW28YSoVatbN2yZXbidxxdxG/GDHc2U8+eUCHlGmXGJMzy5cupVasW9erVw5vF0gRIVdmwYQNbtmyhUaNGuy0TkS9LO4Oofcr5sXChm4b0zjvd4/btXaVXSxLGxLVjxw5LEgkkItSrV6/cW3D2SRfPzp3w0ENu5HzRIjdQbYzZI5YkEiuI451yYxQJM38+XHGFGzm/+GJ49lnYb7+wozLGmISzFkVxKlaEzZvhrbfgjTcsSRiTwsaNG4eIsGjRot+fmzZtGueee+5u61199dWMHTsWcAPx/fv3p0mTJhx55JG0adOG999/v8yxPPzwwzRu3JimTZsyadKkIteZM2cObdu25eijjyY7O5tZs2bttnzVqlXUrFmTJ554oszx+GGJItonn0Dfvu5+06awZAlceGG4MRljymz06NGcfPLJjBkzxvdr7rnnHtauXUtubi65ubm88847bNmypUxxLFiwgDFjxjB//nw++OADrr/+egoKCv6wXr9+/RgwYABz5sxh4MCB9OvXb7flffr04ayzzipTLHvCup4AtmyB/v1h6FBo1Mjdr1/fivgZU45uvrn8r4E6+mgYNCj+Olu3buXTTz9l6tSpnH/++dx3330lbnf79u2MGDGC5cuXU6VKFQD2228/LrnkkjLF+/bbb9O1a1eqVKlCo0aNaNy4MbNmzeKEE07YbT0R4eeffwZg8+bNHHjggb8vGz9+PIcddhg1atQoUyx7wj4J338fevSAvDz3l/zAA5DAX4AxJljjx4+nU6dOHH744ey999589dVXtG7dOu5rli5dSsOGDaldu3aJ2+/Tpw9Tp079w/Ndu3alf//+uz333Xff0bZt298fN2jQgO++++4Prx00aBBnnnkmffv2ZdeuXcycOROAbdu28eijj/Lhhx8mrNsJMj1RbNkCV10F++7r5o6I+gUaY8pXSd/8gzJ69GhuvvlmwH14jx49mtatWxd7dtCenjX09NNP+163qOvWitrfsGHDePrpp+nSpQtvvPEG1157LVOmTGHAgAH06dOHmgkuNpp5iUIVJk2C00+HWrVgyhQ3qZDXvDTGpI8NGzbw8ccfk5ubi4hQUFCAiPDYY49Rr149Nm3atNv6GzdupH79+jRu3JhVq1axZcsWatWqFXcfe9KiaNCgAatXr/79cV5e3m7dShEvvfQSgwcPBuDiiy+mW7duAHz++eeMHTuWfv368dNPP1GhQgWqVq1Kr169/B2Q0lLVlLrVrHmsltqaNaoXXKAKqi+9VPrtGGN8WbBgQaj7Hz58uHbv3n2359q3b68zZszQHTt26KGHHvp7jCtWrNCGDRvqTz/9pKqqt912m1599dX666+/qqrqmjVr9JVXXilTPLm5udqyZUvdsWOHLlu2TBs1aqT5+fl/WO+II47QqVOnqqrqlClTtHXr1n9YZ8CAAfr4448XuZ+ijjswW0v5uZsZLQpVePFFuOUW+PVXeOwxK+JnTAYYPXr0H77Vd+nShddff5127drx6quvcs0117Bjxw4qVarEyJEjqVOnDgAPPPAAd999N82bN6dq1arUqFGDgQMHlimeFi1acMkll9C8eXMqVqzIkCFDyMrKAqBbt2707NmT7OxsRowYwU033UR+fj5Vq1YlJyenTPstq8yo9dSjh5vtqH17V8SvSZNggjPG7GbhwoU0a9Ys7DAyTlHHvSy1ntK3RVFQ4EpwVK3qrrA+5hg3eYXVZzLGmD2Snp+a8+e7GeYiRfzatbNKr8YYU0rp9cn5229w//2u9bB0KRx3XNgRGZPxUq17O9UFcbzTp+tp3jy4/HL3s2tXeOYZ2GefsKMyJqNVrVqVDRs2WKnxBFFvPoqq5Twdc/okisqVYft2ePttOP/8sKMxxuCuG8jLy2PdunVhh5IxIjPclafUThTTp8OECfDkk66I3+LF4J1qZowJX6VKlf4w05pJPYGOUYhIJxFZLCJLRaR/EctFRJ7xls8VkfgFWCJ+/tnNW33KKTB+PKxf7563JGGMMeUusEQhIlnAEOAsoDlwmYg0j1ntLKCJd+sODCtpuzXzN0OLFu66iFtucWMS9euXc/TGGGMigmxRtAGWquoyVf0NGAN0jlmnM/Cyd4X5Z0BdETkg3kb3/3UF1Knjivg9+SRUrx5I8MYYY5wgxygOAlZHPc4DjvexzkHA2uiVRKQ7rsUB8KvMn59rlV4BqA+sDzuIJGHHopAdi0J2LAo1Le0Lg0wURZ0LF3uCr591UNUcIAdARGaX9jL0dGPHopAdi0J2LArZsSgkIntY+6hQkF1PecDBUY8bAGtKsY4xxpgQBZkovgCaiEgjEakMdAUmxKwzAbjKO/upLbBZVdfGbsgYY0x4Aut6UtV8EekFTAKygH+q6nwR6ektHw5MBM4GlgLbgWt8bDrcervJxY5FITsWhexYFLJjUajUxyLlyowbY4xJrPQqCmiMMabcWaIwxhgTV9ImisDKf6QgH8ficu8YzBWRmSLSKow4E6GkYxG13nEiUiAiFyUyvkTycyxE5BQRmSMi80VkeqJjTBQf/yN1ROQdEfnGOxZ+xkNTjoj8U0R+FJHcYpaX7nOztJNtB3nDDX7/DzgMqAx8AzSPWeds4H3ctRhtgc/DjjvEY3EisJd3/6xMPhZR632MO1niorDjDvHvoi6wAGjoPd437LhDPBZ3Ao969/cBNgKVw449gGPRHmgN5BazvFSfm8naogik/EeKKvFYqOpMVd3kPfwMdz1KOvLzdwHQG/g38GMig0swP8fir8BbqroKQFXT9Xj4ORYK1BI3KUZNXKLIT2yYwVPVGbj3VpxSfW4ma6IorrTHnq6TDvb0fV6L+8aQjko8FiJyEHAhMDyBcYXBz9/F4cBeIjJNRL4UkasSFl1i+TkWzwHNcBf0zgNuUtVdiQkvqZTqczNZ56Mot/IfacD3+xSRU3GJ4uRAIwqPn2MxCLhdVQvSfEY1P8eiInAs0BGoBvxXRD5T1SVBB5dgfo7FmcAc4DTgT8CHIvKJqv4ccGzJplSfm8maKKz8RyFf71NEWgIjgbNUdUOCYks0P8ciGxjjJYn6wNkikq+q4xMSYeL4/R9Zr6rbgG0iMgNoBaRbovBzLK4BHlHXUb9URJYDRwCzEhNi0ijV52aydj1Z+Y9CJR4LEWkIvAVcmYbfFqOVeCxUtZGqHqqqhwJjgevTMEmAv/+Rt4F2IlJRRKrjqjcvTHCcieDnWKzCtawQkf1wlVSXJTTK5FCqz82kbFFocOU/Uo7PY3EvUA8Y6n2Tztc0rJjp81hkBD/HQlUXisgHwFxgFzBSVYs8bTKV+fy7uB8YJSLzcN0vt6tq2pUfF5HRwClAfRHJAwYAlaBsn5tWwsMYY0xcydr1ZIwxJklYojDGGBOXJQpjjDFxWaIwxhgTlyUKY4wxcVmiMEnJq/w6J+p2aJx1t5bD/kaJyHJvX1+JyAml2MZIEWnu3b8zZtnMssbobSdyXHK9aqh1S1j/aBE5uzz2bTKXnR5rkpKIbFXVmuW9bpxtjALeVdWxInIG8ISqtizD9socU0nbFZGXgCWq+mCc9a8GslW1V3nHYjKHtShMShCRmiLykfdtf56I/KFqrIgcICIzor5xt/OeP0NE/uu99k0RKekDfAbQ2HvtLd62ckXkZu+5GiLynje3Qa6IXOo9P01EskXkEaCaF8dr3rKt3s9/RX/D91oyXUQkS0QeF5EvxM0T0MPHYfkvXkE3EWkjbi6Sr72fTb2rlAcCl3qxXOrF/k9vP18XdRyN+YOw66fbzW5F3YACXBG3OcA4XBWB2t6y+rgrSyMt4q3ez1uBu7z7WUAtb90ZQA3v+duBe4vY3yi8uSuAi4HPcQX15gE1cKWp5wPHAF2AEVGvreP9nIb79v57TFHrRGK8EHjJu18ZV8mzGtAduNt7vgowG2hURJxbo97fm0An73FtoKJ3/8/Av737VwPPRb3+IeAK735dXN2nGmH/vu2W3LekLOFhDPCLqh4deSAilYCHRKQ9rhzFQcB+wPdRr/kC+Ke37nhVnSMiHYDmwKdeeZPKuG/iRXlcRO4G1uGq8HYExqkrqoeIvAW0Az4AnhCRR3HdVZ/swft6H3hGRKoAnYAZqvqL193VUgpn5KsDNAGWx7y+mojMAQ4FvgQ+jFr/JRFpgqsGWqmY/Z8BnC8ifb3HVYGGpGcNKFNOLFGYVHE5bmayY1V1p4iswH3I/U5VZ3iJ5BzgFRF5HNgEfKiql/nYx22qOjbyQET+XNRKqrpERI7F1cx5WEQmq+pAP29CVXeIyDRc2etLgdGR3QG9VXVSCZv4RVWPFpE6wLvADcAzuFpGU1X1Qm/gf1oxrxegi6ou9hOvMWBjFCZ11AF+9JLEqcAhsSuIyCHeOiOAF3BTQn4GnCQikTGH6iJyuM99zgAu8F5TA9dt9ImIHAhsV9VXgSe8/cTa6bVsijIGV4ytHa6QHd7P6yKvEZHDvX0WSVU3AzcCfb3X1AG+8xZfHbXqFlwXXMQkoLd4zSsROaa4fRgTYYnCpIrXgGwRmY1rXSwqYp1TgDki8jVuHGGwqq7DfXCOFpG5uMRxhJ8dqupXuLGLWbgxi5Gq+jVwFDDL6wK6C3igiJfnAHMjg9kxJuPmNp6ibupOcHOJLAC+EpFc4HlKaPF7sXyDK6v9GK518ylu/CJiKtA8MpiNa3lU8mLL9R4bE5edHmuMMSYua1EYY4yJyxKFMcaYuCxRGGOMicsShTHGmLgsURhjjInLEoUxxpi4LFEYY4yJ6/8BR0y5wTg7OIIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute predicted probabilities\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "probs = nb_model.predict_proba(X_val_tfidf)\n",
    "\n",
    "# Evaluate the classifier\n",
    "evaluate_roc(probs, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf104fcc",
   "metadata": {},
   "source": [
    "# 5. BERT微调\n",
    "\n",
    "## 5.1 标记化和输入格式调整\n",
    "\n",
    "- 包括删除实体（例如@united）和一些特殊字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e821b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13b01f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  @united I'm having issues. Yesterday I rebooked for 24 hours after I was supposed to fly, now I can't log on &amp; check in. Can you help?\n",
      "Processed:  I'm having issues. Yesterday I rebooked for 24 hours after I was supposed to fly, now I can't log on & check in. Can you help?\n"
     ]
    }
   ],
   "source": [
    "# Print sentence 0\n",
    "print('Original: ', X[0])\n",
    "print('Processed: ', text_preprocessing(X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b6324",
   "metadata": {},
   "source": [
    "## 5.2 BERT 分词器\n",
    "\n",
    "- 应用预训练的 BERT，我们必须使用库提供的分词器。\n",
    "- 这是因为 (1) 模型具有特定的、固定的词汇表，\n",
    "- (2) BERT 分词器具有处理词汇表外单词的特定方式。\n",
    "\n",
    "- 需要在每个句子的开头和结尾添加特殊标记，\n",
    "- 将所有句子填充和截断为单个恒定长度，\n",
    "- 并使用“注意掩码（attention mask）”明确指定填充标记。\n",
    "\n",
    "- encode_plus方法将：\n",
    "\n",
    "(1) 将我们的文本拆分为标记，\n",
    "\n",
    "(2) 添加特殊[CLS]和[SEP]标记，\n",
    "\n",
    "(3) 将这些标记转换为标记器词汇的索引，\n",
    "\n",
    "(4) 填充或截断句子到最大长度，以及\n",
    "\n",
    "(5) 创建注意掩码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a57e8c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fcf3df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1ce540a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  68\n"
     ]
    }
   ],
   "source": [
    "#  specify the maximum length of our sentences.\n",
    "\n",
    "# Concatenate train data and test data\n",
    "all_tweets = np.concatenate([data.tweet.values, test_data.tweet.values])\n",
    "\n",
    "# Encode our concatenated data\n",
    "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n",
    "\n",
    "# Find the maximum length\n",
    "max_len = max([len(sent) for sent in encoded_tweets])\n",
    "print('Max length: ', max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0082f7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2227: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  @united I'm having issues. Yesterday I rebooked for 24 hours after I was supposed to fly, now I can't log on &amp; check in. Can you help?\n",
      "Token IDs:  [101, 1045, 1005, 1049, 2383, 3314, 1012, 7483, 1045, 2128, 8654, 2098, 2005, 2484, 2847, 2044, 1045, 2001, 4011, 2000, 4875, 1010, 2085, 1045, 2064, 1005, 1056, 8833, 2006, 1004, 4638, 1999, 1012, 2064, 2017, 2393, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "# 开始标记data\n",
    "\n",
    "# Specify `MAX_LEN`\n",
    "MAX_LEN = 64\n",
    "\n",
    "# Print sentence 0 and its encoded token ids\n",
    "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
    "print('Original: ', X[0])\n",
    "print('Token IDs: ', token_ids)\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0d3d3",
   "metadata": {},
   "source": [
    "### 创建 PyTorch 数据加载器\n",
    "\n",
    "使用 torch DataLoader 类为我们的数据集创建一个迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f62e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dddd701",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "\n",
    "#### 创建 BertClassifier \n",
    "\n",
    "- BERT-base 由 12 个transformer 层组成\n",
    "- 每个transformer 层接受一个token embeddings 列表，并在输出上产生相同数量的具有相同隐藏大小（或维度）的嵌入\n",
    "- 令最后一个变换层的输出[CLS]被用作序列的特征来馈送分类器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "51c15122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.8 ms, sys: 18.8 ms, total: 46.5 ms\n",
      "Wall time: 72.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2591b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 设定优化器和学习率\n",
    "\n",
    "参数如下\n",
    "- Batch size: 16 or 32\n",
    "- Learning rate (Adam): 5e-5, 3e-5 or 2e-5\n",
    "- Number of epochs: 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9bea51ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 训练循环\n",
    "\n",
    "训练：\n",
    "\n",
    "从数据加载器中解压我们的数据并将数据加载到 GPU 上\n",
    "将上一次计算的梯度归零\n",
    "执行前向传递以计算 logits 和损失\n",
    "执行反向传递以计算梯度 ( loss.backward())\n",
    "将梯度的范数剪裁为 1.0 以防止“梯度爆炸”\n",
    "更新模型的参数 ( optimizer.step())\n",
    "更新学习率 ( scheduler.step())\n",
    "评估：\n",
    "\n",
    "解压我们的数据并加载到 GPU\n",
    "前传\n",
    "计算验证集的损失和准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "526fd35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a19d50d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a92ad4d2e614c699332e8d07af93de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.650564   |     -      |     -     |  873.48  \n",
      "   1    |   40    |   0.577186   |     -      |     -     |  715.62  \n",
      "   1    |   60    |   0.460372   |     -      |     -     |  563.97  \n",
      "   1    |   80    |   0.486488   |     -      |     -     |  1344.41 \n",
      "   1    |   95    |   0.493380   |     -      |     -     |  417.04  \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.536911   |  0.426013  |   80.68   |  4001.69 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.319017   |     -      |     -     |  714.97  \n",
      "   2    |   40    |   0.296759   |     -      |     -     |  588.53  \n",
      "   2    |   60    |   0.297036   |     -      |     -     |  583.95  \n",
      "   2    |   80    |   0.308690   |     -      |     -     |  602.54  \n",
      "   2    |   95    |   0.280645   |     -      |     -     |  495.65  \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.301654   |  0.425967  |   81.42   |  3043.67 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1ea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 评估验证集\n",
    "\n",
    "类似于训练循环的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cdaacc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3a466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9012\n",
      "Accuracy: 81.47%\n"
     ]
    }
   ],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, val_dataloader)\n",
    "\n",
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2c4738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
